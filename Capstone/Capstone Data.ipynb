{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Data Collection\n",
    "This Python Notebook deals exclusively with data collection, compilation and brief cleaning. \n",
    "\n",
    "Data sources:\n",
    "The majority of indicator data for this project was gathered from the World Bank. The corruption perception index was the only externally gathered indicator, from Transparency International.\n",
    "\n",
    "Procedure:\n",
    "The procedure for each indicator imported/collected below (excluding CPI), will be fairly similar.\n",
    "1. Import data from file as dataframe\n",
    "2. Assign to variable the missing value of indicator\n",
    "3. Drop redundant columns\n",
    "4. Rename all columns to include indicator-specific prefix\n",
    "5. Subset and reassign only required columns for merging\n",
    "\n",
    "This procedure was different for the Corruption Perception Index (CPI) as data was collected through different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d/anaconda/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDP Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike other indicators, the GDP information has an additional table that contains some interesting additional information (such as income group and region). The use of such information may assist us in EDA and imputation later, and thus, we will include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign file path to variables\n",
    "GDP1path = './datasets/original_files/API_NY.GDP.MKTP.CD_DS2_en_csv_v2.csv'\n",
    "GDP2path = './datasets/original_files/Metadata_Country_API_NY.GDP.MKTP.CD_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in data\n",
    "GDP = pd.read_csv(GDP1path, skiprows=3)\n",
    "GDP2 = pd.read_csv(GDP2path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1961</th>\n",
       "      <th>1962</th>\n",
       "      <th>1963</th>\n",
       "      <th>1964</th>\n",
       "      <th>1965</th>\n",
       "      <th>...</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>Unnamed: 61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>ABW</td>\n",
       "      <td>GDP (current US$)</td>\n",
       "      <td>NY.GDP.MKTP.CD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.791961e+09</td>\n",
       "      <td>2.498933e+09</td>\n",
       "      <td>2.467704e+09</td>\n",
       "      <td>2.584464e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>GDP (current US$)</td>\n",
       "      <td>NY.GDP.MKTP.CD</td>\n",
       "      <td>5.377778e+08</td>\n",
       "      <td>5.488889e+08</td>\n",
       "      <td>5.466667e+08</td>\n",
       "      <td>7.511112e+08</td>\n",
       "      <td>8.000000e+08</td>\n",
       "      <td>1.006667e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019053e+10</td>\n",
       "      <td>1.248694e+10</td>\n",
       "      <td>1.593680e+10</td>\n",
       "      <td>1.793024e+10</td>\n",
       "      <td>2.053654e+10</td>\n",
       "      <td>2.004633e+10</td>\n",
       "      <td>2.005019e+10</td>\n",
       "      <td>1.970299e+10</td>\n",
       "      <td>1.946902e+10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>GDP (current US$)</td>\n",
       "      <td>NY.GDP.MKTP.CD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.417803e+10</td>\n",
       "      <td>7.549238e+10</td>\n",
       "      <td>8.247091e+10</td>\n",
       "      <td>1.041159e+11</td>\n",
       "      <td>1.153984e+11</td>\n",
       "      <td>1.249121e+11</td>\n",
       "      <td>1.267769e+11</td>\n",
       "      <td>1.029622e+11</td>\n",
       "      <td>8.963316e+10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>GDP (current US$)</td>\n",
       "      <td>NY.GDP.MKTP.CD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.288135e+10</td>\n",
       "      <td>1.204421e+10</td>\n",
       "      <td>1.192695e+10</td>\n",
       "      <td>1.289087e+10</td>\n",
       "      <td>1.231978e+10</td>\n",
       "      <td>1.278103e+10</td>\n",
       "      <td>1.321986e+10</td>\n",
       "      <td>1.139037e+10</td>\n",
       "      <td>1.192689e+10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>AND</td>\n",
       "      <td>GDP (current US$)</td>\n",
       "      <td>NY.GDP.MKTP.CD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.001201e+09</td>\n",
       "      <td>3.650083e+09</td>\n",
       "      <td>3.346517e+09</td>\n",
       "      <td>3.427023e+09</td>\n",
       "      <td>3.146152e+09</td>\n",
       "      <td>3.248925e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country Name Country Code     Indicator Name  Indicator Code          1960  \\\n",
       "0        Aruba          ABW  GDP (current US$)  NY.GDP.MKTP.CD           NaN   \n",
       "1  Afghanistan          AFG  GDP (current US$)  NY.GDP.MKTP.CD  5.377778e+08   \n",
       "2       Angola          AGO  GDP (current US$)  NY.GDP.MKTP.CD           NaN   \n",
       "3      Albania          ALB  GDP (current US$)  NY.GDP.MKTP.CD           NaN   \n",
       "4      Andorra          AND  GDP (current US$)  NY.GDP.MKTP.CD           NaN   \n",
       "\n",
       "           1961          1962          1963          1964          1965  \\\n",
       "0           NaN           NaN           NaN           NaN           NaN   \n",
       "1  5.488889e+08  5.466667e+08  7.511112e+08  8.000000e+08  1.006667e+09   \n",
       "2           NaN           NaN           NaN           NaN           NaN   \n",
       "3           NaN           NaN           NaN           NaN           NaN   \n",
       "4           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "      ...               2008          2009          2010          2011  \\\n",
       "0     ...       2.791961e+09  2.498933e+09  2.467704e+09  2.584464e+09   \n",
       "1     ...       1.019053e+10  1.248694e+10  1.593680e+10  1.793024e+10   \n",
       "2     ...       8.417803e+10  7.549238e+10  8.247091e+10  1.041159e+11   \n",
       "3     ...       1.288135e+10  1.204421e+10  1.192695e+10  1.289087e+10   \n",
       "4     ...       4.001201e+09  3.650083e+09  3.346517e+09  3.427023e+09   \n",
       "\n",
       "           2012          2013          2014          2015          2016  \\\n",
       "0           NaN           NaN           NaN           NaN           NaN   \n",
       "1  2.053654e+10  2.004633e+10  2.005019e+10  1.970299e+10  1.946902e+10   \n",
       "2  1.153984e+11  1.249121e+11  1.267769e+11  1.029622e+11  8.963316e+10   \n",
       "3  1.231978e+10  1.278103e+10  1.321986e+10  1.139037e+10  1.192689e+10   \n",
       "4  3.146152e+09  3.248925e+09           NaN           NaN           NaN   \n",
       "\n",
       "   Unnamed: 61  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign values with sum of missing values: missgdp\n",
    "missgdp = GDP.iloc[:,2:].isnull().sum()\n",
    "\n",
    "#Print head\n",
    "GDP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Subset GDP2 in preparation for merge\n",
    "conc = GDP2.loc[:,['Country Code', 'Region', 'IncomeGroup']]\n",
    "\n",
    "# Merge GDP and conc to single df\n",
    "GDP_com = pd.merge(GDP, conc, how='left', on='Country Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-arrange the columns\n",
    "GDP_com = GDP_com[['Country Name', 'Country Code', 'Region', 'IncomeGroup', '1960', '1961', '1962', '1963', '1964',\n",
    "                   '1965', '1966', '1967', '1968', '1969', '1970', '1971','1972', '1973', '1974', '1975', '1976',\n",
    "                   '1977', '1978', '1979', '1980', '1981', '1982', '1983','1984', '1985', '1986', '1987', '1988',\n",
    "                   '1989', '1990', '1991','1992', '1993', '1994', '1995','1996', '1997', '1998', '1999', '2000',\n",
    "                   '2001', '2002', '2003', '2004', '2005', '2006', '2007','2008', '2009', '2010', '2011', '2012',\n",
    "                   '2013', '2014', '2015','2016'\n",
    "                  ]]\n",
    "# Print head\n",
    "GDP_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns by adding prefix\n",
    "for col in GDP_com.iloc[:,4:].columns:\n",
    "    GDP_com.rename(columns={col: 'GDPtot_' + str(col)}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print head of completed merge and rename\n",
    "GDP_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export file to CSV\n",
    "GDP_com.to_csv('./datasets/GDP.csv', index=False)\n",
    "# Print shape\n",
    "GDP_com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe and check contents\n",
    "GDPcheck = pd.read_csv('./datasets/GDP.csv')\n",
    "GDPcheck.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Life Expectancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign file path to variable\n",
    "lifepath = './datasets/original_files/API_SP.DYN.LE00.IN_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in file as dataframe\n",
    "life = pd.read_csv(lifepath, skiprows=3)\n",
    "\n",
    "# Assign sum of missing values: missLE\n",
    "missLE = life.iloc[:,4:].isnull().sum()\n",
    "\n",
    "# Print head\n",
    "life.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column as there's no information for year 2016\n",
    "life.drop(labels='2016', axis=1, inplace=True)\n",
    "\n",
    "#Drop duplicate and misc columns\n",
    "life.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename each column with indicator values to prefixed name\n",
    "for i in life.iloc[:,2:]:\n",
    "    life.rename(columns={i: 'LifeExp_' + str(i)}, inplace=True)\n",
    "    \n",
    "# Print modified column names\n",
    "life.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export file to csv\n",
    "life.to_csv('./datasets/life_expectancy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check contents of exported file and print\n",
    "lifecheck = pd.read_csv('./datasets/life_expectancy.csv')\n",
    "\n",
    "# Print head\n",
    "lifecheck.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our unemployment data into our df\n",
    "unemp = pd.read_csv('./datasets/original_files/API_SL.UEM.TOTL.ZS_DS2_en_csv_v2.csv', skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "unemp.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign sum of missing values: missUnemp\n",
    "missUnemp = unemp.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Print head of df\n",
    "unemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename indicator value columns to include prefix\n",
    "for i in unemp.iloc[:,2:]:\n",
    "    unemp.rename(columns={i: 'unemp_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head of df\n",
    "unemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "unemp.to_csv('./datasets/unemploymentpercent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in from csv to check contents\n",
    "unempcheck = pd.read_csv('./datasets/unemploymentpercent.csv')\n",
    "unempcheck.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDP Per Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in csv as df\n",
    "GDPpercap = pd.read_csv('./datasets/original_files/API_NY.GDP.PCAP.CD_DS2_en_csv_v2.csv', skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "GDPpercap.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign sum of missing values: missGDPpc\n",
    "missGDPpc = GDPpercap.iloc[:,2:].isnull().sum()\n",
    "\n",
    "#Print head\n",
    "GDPpercap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename indicator value columns to include prefix\n",
    "for i in GDPpercap.iloc[:,2:]:\n",
    "    GDPpercap.rename(columns={i: 'GDPpc_'+str(i)}, inplace=True)\n",
    "    \n",
    "# Print head\n",
    "GDPpercap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "GDPpercap.to_csv('./datasets/gdp_per_cap.csv', index=False)\n",
    "\n",
    "# Read csv as df to check\n",
    "pd.read_csv('./datasets/gdp_per_cap.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign filepath of csv: filepath\n",
    "filepath = './datasets/original_files/API_SE.ENR.PRSC.FM.ZS_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv as dataframe and assign: school\n",
    "school = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "school.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name', '2016'], axis=1, inplace=True)\n",
    "\n",
    "# Assign sum of missing values: missSchool\n",
    "missSchool = school.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator values with prefix included\n",
    "for i in school.iloc[:,2:]:\n",
    "    school.rename(columns={i: 'school_enrl_'+str(i)}, inplace=True)\n",
    "\n",
    "\n",
    "school.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export school to csv\n",
    "school.to_csv('./datasets/school.csv', index=False)\n",
    "\n",
    "# Read csv and check contents\n",
    "test = pd.read_csv('./datasets/school.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult Literacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path: filepath\n",
    "filepath = './datasets/original_files/API_SE.ADT.LITR.ZS_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv as df and assign: liter\n",
    "liter = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "liter.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name', '2016'], axis=1, inplace=True)\n",
    "\n",
    "# Assign sum of missing values: missLiter\n",
    "missLiter = liter.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator value columns to include prefix\n",
    "for i in liter.iloc[:,2:]:\n",
    "    liter.rename(columns={i: 'AdulLit_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "liter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "liter.to_csv('./datasets/Adult_Literacy.csv', index=False)\n",
    "\n",
    "# Read exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/Adult_Literacy.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undernourishment Prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign file path of csv: filepath\n",
    "filepath = './datasets/original_files/API_SN.ITK.DEFC.ZS_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv as df and assign: nour\n",
    "nour = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "nour.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name', '2016'], axis=1, inplace=True)\n",
    "\n",
    "# Assign sum of missing values: missNour\n",
    "missNour = nour.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator value columns to include prefix\n",
    "for i in nour.iloc[:,2:]:\n",
    "    nour.rename(columns={i: 'UndNour_'+str(i)}, inplace=True)\n",
    "    \n",
    "# Print head\n",
    "nour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "nour.to_csv('./datasets/Under_Nourish.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/Under_Nourish.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_FP.CPI.TOTL.ZG_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: inflat\n",
    "inflat = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "inflat.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missInfl\n",
    "missInfl = inflat.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in inflat.iloc[:,2:]:\n",
    "    inflat.rename(columns={i: 'Inflat_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "inflat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "inflat.to_csv('./datasets/inflation.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/inflation.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electricity Power Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_EG.USE.ELEC.KH.PC_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: elec\n",
    "elec = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "elec.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missElec\n",
    "missElec = elec.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in elec.iloc[:,2:]:\n",
    "    elec.rename(columns={i: 'Elec_'+str(i)}, inplace=True)\n",
    "    \n",
    "# Print head\n",
    "elec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "elec.to_csv('./datasets/elec_consumption.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/elec_consumption.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fossil Fuel Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_EG.USE.COMM.FO.ZS_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: fossil\n",
    "fossil = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "fossil.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name', '2016'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missFossil\n",
    "missFossil = fossil.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in fossil.iloc[:,2:]:\n",
    "    fossil.rename(columns={i: 'FossilCon_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "fossil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "fossil.to_csv('./datasets/fossil_consumption.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/fossil_consumption.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infant Mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_SP.DYN.IMRT.IN_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: infant\n",
    "infant = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "infant.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name', '2016'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missInfant\n",
    "missInfant = infant.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in infant.iloc[:,2:]:\n",
    "    infant.rename(columns={i: 'Infant_'+str(i)}, inplace=True)\n",
    "    \n",
    "# Print head\n",
    "infant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "infant.to_csv('./datasets/infant_mortality.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/infant_mortality.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co2 Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_EN.ATM.CO2E.PC_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: emissions\n",
    "emissions = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "emissions.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name', '2014', '2015', '2016'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missEmis\n",
    "missEmis = emissions.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in emissions.iloc[:,2:]:\n",
    "    emissions.rename(columns={i: 'co2Emis_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "emissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "emissions.to_csv('./datasets/emissions.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/emissions.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electricity Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_EG.ELC.ACCS.ZS_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: elecAccess\n",
    "elecAccess = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "elecAccess.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name', '2016'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missEleAcc\n",
    "missEleAcc = elecAccess.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in elecAccess.iloc[:,2:]:\n",
    "    elecAccess.rename(columns={i: 'ElecAcc_'+str(i)}, inplace=True)\n",
    "\n",
    "    \n",
    "# Print head\n",
    "elecAccess.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "elecAccess.to_csv('./datasets/elec_access.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/elec_access.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prop of Women in parliament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_SG.GEN.PARL.ZS_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: WomenProp\n",
    "WomenProp = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "WomenProp.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missWomenProp\n",
    "missWomenProp = WomenProp.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in WomenProp.iloc[:,2:]:\n",
    "    WomenProp.rename(columns={i: 'WomenProp_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "WomenProp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "WomenProp.to_csv('./datasets/women_parliament.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/women_parliament.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanitation Access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_SH.STA.ACSN_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: sanitation\n",
    "sanitation = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "sanitation.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missSanit\n",
    "missSanit = sanitation.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in sanitation.iloc[:,2:]:\n",
    "    sanitation.rename(columns={i: 'ImpSanit_Acc_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "sanitation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "sanitation.to_csv('./datasets/sanitation_acc.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/sanitation_acc.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tax on comm profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_IC.TAX.TOTL.CP.ZS_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: taxcomm\n",
    "taxcomm = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "taxcomm.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missTaxComm\n",
    "missTaxComm = taxcomm.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in taxcomm.iloc[:,2:]:\n",
    "    taxcomm.rename(columns={i: 'CommTotTax_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "taxcomm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "taxcomm.to_csv('./datasets/commercial_tax.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/commercial_tax.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elec Production from oil gas coal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_EG.ELC.FOSL.ZS_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: Elec_OGC\n",
    "Elec_OGC = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "Elec_OGC.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missElec_OGC\n",
    "missElec_OGC = Elec_OGC.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in Elec_OGC.iloc[:,2:]:\n",
    "    Elec_OGC.rename(columns={i: 'ElecOGC_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "Elec_OGC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "Elec_OGC.to_csv('./datasets/Elec_OGC.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/Elec_OGC.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ores and Metals exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_TX.VAL.MMTL.ZS.UN_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: \n",
    "OreMetals = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "OreMetals.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missOreMetals\n",
    "missOreMetals = OreMetals.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in OreMetals.iloc[:,2:]:\n",
    "    OreMetals.rename(columns={i: 'OreMetals_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "OreMetals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "OreMetals.to_csv('./datasets/OreMetals_exports.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/OreMetals_exports.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High tech exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_TX.VAL.TECH.MF.ZS_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: \n",
    "techexp = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "techexp.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missTechExp\n",
    "missTechExp = techexp.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in techexp.iloc[:,2:]:\n",
    "    techexp.rename(columns={i: 'TechExp_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "techexp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "techexp.to_csv('./datasets/tech_exports.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/tech_exports.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proc Business Register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign csv file path to variable: filepath\n",
    "filepath = './datasets/original_files/API_IC.REG.PROC_DS2_en_csv_v2.csv'\n",
    "\n",
    "# Read in csv file as df: BusReg\n",
    "BusReg = pd.read_csv(filepath, skiprows=3)\n",
    "\n",
    "# Drop redundant rows\n",
    "BusReg.drop(labels=['Unnamed: 61', 'Indicator Code', 'Indicator Name'], axis=1, inplace=True)\n",
    "\n",
    "# Assign missing values sum: missBusReg\n",
    "missBusReg = BusReg.iloc[:,2:].isnull().sum()\n",
    "\n",
    "# Rename indicator columns by adding prefix\n",
    "for i in BusReg.iloc[:,2:]:\n",
    "    BusReg.rename(columns={i: 'BusReg_'+str(i)}, inplace=True)\n",
    "\n",
    "# Print head\n",
    "BusReg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Export df to csv\n",
    "BusReg.to_csv('./datasets/BusReg_procedure.csv', index=False)\n",
    "\n",
    "# Read in exported csv as df to check contents\n",
    "test = pd.read_csv('./datasets/BusReg_procedure.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corruption Perception Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Corruption Perception Index is not as accessible as previously collected data. \n",
    "\n",
    "* The data for 2005-2009 is only available in HTML table form on their website. \n",
    "* The more recent data is stored within files such as csv or excel formats. \n",
    "* There is a lack of consistency between years, and as such, cleaning to ensure that years are comparable will need to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping CPI data\n",
    "The first 4 years will need to be scraped.\n",
    "The procedure for scraping the first 4 years will be fairly similar.\n",
    "1. Instruct driver to go to URL\n",
    "2. Find table element on page\n",
    "3. Extract outerHTML from table element\n",
    "4. Parse HTML table as dataframe\n",
    "5. Resolve any scraping issues\n",
    "6. Rename columns to ensure consistency\n",
    "7. Subset table/drop redundant rows\n",
    "8. Save and export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize driver for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages for scraping\n",
    "import urllib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Please place the filepath of your driver below instead\n",
    "driverPath = '/users/D/Desktop/Resources/chromedriver'\n",
    "\n",
    "# Assign to driver the Chrome webdriver\n",
    "driver = webdriver.Chrome(executable_path=driverPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPI 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instruct browser to retrieve URL\n",
    "driver.get('https://www.transparency.org/research/cpi/cpi_2005')\n",
    "\n",
    "# Find table by element name: table2005\n",
    "table2005 = driver.find_element_by_class_name('simple')\n",
    "\n",
    "# Retrieve the outerHTML from the object and store: extract\n",
    "extract = pd.read_html(table2005.get_attribute('outerHTML'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve table from first item of extract\n",
    "CPI2005 = extract[0]\n",
    "\n",
    "# Print head\n",
    "CPI2005.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping issues\n",
    "* During the scrape, an issue arose where shared ranks between countries were not being read appropriately. (Seen above index 2: New Zealand)\n",
    "* To fix this issue, we have created the loop below to iterate over rows and shift if the issue is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through rows that have non-numeric values in the Rank column\n",
    "for i, row in CPI2005[CPI2005['Rank'].str.isnumeric()== False].iterrows():\n",
    "    # Shift values forward in the actual table to the values of the iterated rows\n",
    "    CPI2005['Surveys Used'][i]= row[3]\n",
    "    CPI2005['Confidence Range'][i]= row[2]\n",
    "    CPI2005['CPI 2005 Score'][i]= row[1]\n",
    "    CPI2005['Country/Territory'][i]= row[0]\n",
    "    \n",
    "    # As country is sharing rank with one above, copy value over\n",
    "    CPI2005['Rank'][i]= CPI2005['Rank'][i-1]\n",
    "    print('Corrected: \\n' +str(CPI2005.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace spaces in columns with _\n",
    "CPI2005.columns = [col.replace(' ', '_') for col in CPI2005.columns]\n",
    "\n",
    "# Subset table and reassign (Drop rows)\n",
    "CPI2005 = CPI2005.iloc[:,1:3]\n",
    "\n",
    "# Print head\n",
    "CPI2005.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CPI 2005 table export\n",
    "CPI2005.to_csv('./datasets/CPI2005.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPI 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instruct browser to retrieve URL\n",
    "driver.get('https://www.transparency.org/research/cpi/cpi_2006/')\n",
    "\n",
    "# Find table by element name: table2006\n",
    "table2006 = driver.find_element_by_class_name('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the outerHTML from the object and store: extract\n",
    "extract = pd.read_html(table2006.get_attribute('outerHTML'))\n",
    "\n",
    "# Retrieve table from first item of extract\n",
    "CPI2006 = extract[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace spaces in columns with _\n",
    ".columns = [col.replace(' ', '_') for col in CPI2006.columns]\n",
    "\n",
    "# Print head\n",
    "CPI2006.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CPI 2006 table export\n",
    "CPI2006.to_csv('./datasets/CPI2006.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPI 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instruct browser to retrieve URL\n",
    "driver.get('https://www.transparency.org/research/cpi/cpi_2007/')\n",
    "\n",
    "# Find table by element name: table\n",
    "table2007 = driver.find_element_by_class_name('simple')\n",
    "\n",
    "# Retrieve the outerHTML from the object and store: extract\n",
    "extract = pd.read_html(table2007.get_attribute('outerHTML'))\n",
    "\n",
    "# Retrieve table from first item of extract\n",
    "CPI2007 = extract[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace spaces in columns with _\n",
    "CPI2007.columns = [col.replace(' ', '_') for col in CPI2007.columns]\n",
    "\n",
    "# Subset and reassign (drop all columns except Country and score)\n",
    "CPI2007 = CPI2007.iloc[:,1:3]\n",
    "\n",
    "# Print head\n",
    "CPI2007.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export table\n",
    "CPI2007.to_csv('./datasets/CPI2007.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPI 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instruct browser to retrieve URL\n",
    "driver.get('https://www.transparency.org/research/cpi/cpi_2008')\n",
    "\n",
    "# Find table by element name: table2008\n",
    "table2008 = driver.find_element_by_class_name('simple')\n",
    "\n",
    "# Retrieve the outerHTML from the object and store: extract\n",
    "extract = pd.read_html(table2008.get_attribute('outerHTML'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve table from first item of extract\n",
    "PI2008 = extract[0]\n",
    "\n",
    "# Replace spaces in columns with _\n",
    "CPI2008.columns = [col.replace(' ', '_') for col in CPI2008.columns]\n",
    "\n",
    "# Subset to only include Country an score\n",
    "CPI2008 = CPI2008.iloc[:,1:3]\n",
    "\n",
    "# export table\n",
    "CPI2008.to_csv('./datasets/CPI2008.csv', index=False, encoding='utf-8')\n",
    "\n",
    "#print head\n",
    "CPI2008.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPI 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use driver to scrape table by class name\n",
    "driver.get('https://www.transparency.org/research/cpi/cpi_2009')\n",
    "table2009 = driver.find_element_by_class_name('simple')\n",
    "\n",
    "# Read the table from html contained within scraped element\n",
    "extract = pd.read_html(table2009.get_attribute('outerHTML'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract table from extract cell\n",
    "CPI2009 = extract[0]\n",
    "\n",
    "# Replace spaces in columns with _\n",
    "CPI2009.columns = [col.replace(' ', '_') for col in CPI2009.columns]\n",
    "\n",
    "# Subset to only country and score\n",
    "CPI2009 = CPI2009.iloc[:,1:3]\n",
    "\n",
    "# Export to csv\n",
    "CPI2009.to_csv('./datasets/CPI2009.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Print head\n",
    "CPI2009.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPI 2010 onwards\n",
    "All future years will be imported directly below from their csv/excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign all paths to their respective year's file path\n",
    "path2010 = './datasets/CPI/CPI+2010+results_pls_standardized_data.xls'\n",
    "path2011 = './datasets/CPI/CPI2011_DataPackage/CPI2011_Results.xls'\n",
    "path2012 = './datasets/CPI/2012_CPI_DataPackage/CPI 2012-Table.csv'\n",
    "path2013 = './datasets/CPI/CPI2013_DataBundle/CPI 2013-Table 1.csv'\n",
    "path2014 = './datasets/CPI/CPI2014_DataBundle/CPI 2014-Table 1.csv'\n",
    "path2015 = './datasets/CPI/CPI 2015/Data/CPI 2015_data.xlsx'\n",
    "path2016 = './datasets/CPI/CPI2016_FullDataSetWithRegionalTables.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in excel file as df skipping first 3 rows\n",
    "CPI2010 = pd.read_excel(path2010, skiprows=3)\n",
    "\n",
    "# Rename country, cpi2010, and no_sources_a columns\n",
    "CPI2010.rename(columns={'country': 'Country/Territory', 'cpi2010': 'CPI_2010_Score', 'no_sources_a': 'Surveys_Used'}, inplace=True)\n",
    "\n",
    "# Drop rank column\n",
    "CPI2010.drop(labels='rank', axis=1, inplace=True)\n",
    "\n",
    "# Replace all spaces in column names to _\n",
    "CPI2010.columns = [col.replace(' ', '_') for col in CPI2010.columns]\n",
    "\n",
    "# subset df and reassign\n",
    "CPI2010 = CPI2010.iloc[:,:2]\n",
    "\n",
    "# Print head\n",
    "CPI2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in excel file as df\n",
    "CPI2011 = pd.read_excel(path2011, skiprows=0)\n",
    "\n",
    "# Rename country & cpi11 columns\n",
    "CPI2011.rename(columns={'country': 'Country/Territory', 'cpi11': 'CPI_2011_Score'}, inplace=True)\n",
    "\n",
    "# Replace all spaces in column names to _\n",
    "CPI2011.columns = [col.replace(' ', '_') for col in CPI2011.columns]\n",
    "\n",
    "# subset df and reassign\n",
    "CPI2011 = CPI2011.iloc[:,:2]\n",
    "\n",
    "# Print head\n",
    "CPI2011.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file as df: CPI2012\n",
    "CPI2012 = pd.read_csv(path2012)\n",
    "\n",
    "# Rename columns\n",
    "CPI2012.rename(columns={'Country / Territory': 'Country/Territory', 'CPI 2012 Score': 'CPI_2012_Score', 'Unnamed: 2': 'Region'}, inplace=True)\n",
    "\n",
    "# Drop redundant columns\n",
    "CPI2012.drop(labels=['Unnamed: 4', 'Country Rank'], axis=1, inplace=True)\n",
    "\n",
    "# Rename columns replacing spaces with _\n",
    "CPI2012.columns = [col.replace(' ', '_') for col in CPI2012.columns]\n",
    "\n",
    "# subset df and reassign\n",
    "CPI2012 = CPI2012.iloc[:,:3]\n",
    "\n",
    "# Print head\n",
    "CPI2012.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file as df: CPI2013\n",
    "CPI2013 = pd.read_csv(path2013)\n",
    "\n",
    "# Rename columns\n",
    "CPI2013.rename(columns={'Country / Territory': 'Country/Territory', 'CPI 2013 Score': 'CPI_2013_Score', 'Unnamed: 2': 'Region'}, inplace=True)\n",
    "\n",
    "# Drop redundant columns\n",
    "CPI2013.drop(labels=['Country Rank.1', 'IFS Code', 'Region', 'Country Rank'], axis=1, inplace=True)\n",
    "\n",
    "# Rename columns replacing spaces with _\n",
    "CPI2013.columns = [col.replace(' ', '_') for col in CPI2013.columns]\n",
    "\n",
    "\n",
    "# Subset df and reassign: 2 options below\n",
    "# Country, code, region score subset\n",
    "#CPI2013 = CPI2013.iloc[:,:9]\n",
    "\n",
    "# Incl: Surveys, Error, CI, Score range\n",
    "CPI2013 = CPI2013.iloc[:,:3]\n",
    "\n",
    "# Print head\n",
    "CPI2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file as df: CPI2014\n",
    "CPI2014 = pd.read_csv(path2014)\n",
    "\n",
    "# Rename columns\n",
    "CPI2014.rename(columns={'Country / Territory': 'Country/Territory', 'CPI 2014 Score': 'CPI_2014_Score'}, inplace=True)\n",
    "\n",
    "# Drop redundant columns\n",
    "CPI2014.drop(labels=['Country Rank.1', 'Country Rank'], axis=1, inplace=True)\n",
    "\n",
    "# Rename columns replacing spaces with _\n",
    "CPI2014.columns = [col.replace(' ', '_') for col in CPI2014.columns]\n",
    "\n",
    "\n",
    "# subset df and reassign: 2 options below\n",
    "# Country, code, region score subset\n",
    "#CPI2014 = CPI2014.iloc[:,:11]\n",
    "\n",
    "# Incl: Surveys, Error, CI, Score range\n",
    "CPI2014 = CPI2014.iloc[:,:4]\n",
    "\n",
    "# Print head\n",
    "CPI2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file as df: CPI2015\n",
    "CPI2015 = pd.read_excel(path2015)\n",
    "\n",
    "# Rename columns\n",
    "CPI2015.rename(columns={'Country': 'Country/Territory', 'CPI2015': 'CPI_2015_Score', 'wbcode': 'WB_Code'}, inplace=True)\n",
    "\n",
    "# Drop redundant columns\n",
    "CPI2015.drop(labels=['Region', 'Rank'], axis=1, inplace=True)\n",
    "\n",
    "# Rename columns replacing spaces with _\n",
    "CPI2015.columns = [col.replace(' ', '_') for col in CPI2015.columns]\n",
    "\n",
    "# Incl: Surveys, Error, CI, Score range\n",
    "CPI2015 = CPI2015.iloc[:,:3]\n",
    "\n",
    "# Print head\n",
    "CPI2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in excel file as df: CPI2016\n",
    "CPI2016 = pd.read_excel(path2016)\n",
    "\n",
    "# Rename columns\n",
    "CPI2016.rename(columns={'Country': 'Country/Territory', 'CPI2016': 'CPI_2016_Score'}, inplace=True)\n",
    "\n",
    "# Drop redundant columns\n",
    "CPI2016.drop(labels=['Region', 'Rank'], axis=1, inplace=True)\n",
    "\n",
    "# Rename columns replacing spaces with _\n",
    "CPI2016.columns = [col.replace(' ', '_') for col in CPI2016.columns]\n",
    "\n",
    "\n",
    "# subset df and reassign\n",
    "# Incl: Surveys, Error, CI, Score range\n",
    "CPI2016 = CPI2016.iloc[:,:3]\n",
    "\n",
    "# Print head\n",
    "CPI2016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling Years of Corruption Perception Index\n",
    "* Will start with 2014 as it holds most information (Includes WB_Code, country, and region data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CPI2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CPI2014.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI2014 and CPI2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge 2014 and 2016 data on the WB_Code columnb\n",
    "CPI = pd.merge(CPI2014, CPI2016, how='outer', on='WB_Code')\n",
    "\n",
    "# Rename columns\n",
    "CPI.rename(columns={'Country/Territory_x': 'Country/Territory'}, inplace=True)\n",
    "\n",
    "# Print shape and head\n",
    "print(CPI.shape)\n",
    "CPI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate through values that are null within country/territory and copy values from merged Country column\n",
    "for i, row in CPI[CPI['Country/Territory'].isnull()].iterrows():\n",
    "    CPI['Country/Territory'][i] = CPI['Country/Territory_y'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop additional country column after merge\n",
    "CPI.drop(labels='Country/Territory_y', axis=1, inplace=True)\n",
    "\n",
    "# Print head\n",
    "CPI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2015 CPI data with overall CPI on WB_Code column\n",
    "CPI = pd.merge(CPI, CPI2015, how='outer', on='WB_Code')\n",
    "\n",
    "# Rename columns changed during merge\n",
    "CPI.rename(columns={'Country/Territory_x': 'Country/Territory'}, inplace=True)\n",
    "\n",
    "# Iterate through values that are null within country/territory and copy values from merged Country column\n",
    "for i, row in CPI[CPI['Country/Territory'].isnull()].iterrows():\n",
    "    CPI['Country/Territory'][i] = CPI['Country/Territory_y'][i]\n",
    "    \n",
    "# Drop extra country column from merge\n",
    "CPI.drop(labels='Country/Territory_y', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint CPI 2014-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Progress\n",
    "CPI.to_csv('./datasets/CPI14-16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load Progress\n",
    "CPI = pd.read_csv('./datasets/CPI14-16.csv', index_col=0)\n",
    "\n",
    "# Print head\n",
    "CPI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2013 data into main CPI df on WB_Code column\n",
    "CPI = pd.merge(CPI, CPI2013, how='outer', on='WB_Code')\n",
    "\n",
    "# Rename country column\n",
    "CPI.rename(columns={'Country/Territory_x': 'Country/Territory'}, inplace=True)\n",
    "\n",
    "# Iterate through values that are null within country/territory and copy values from merged Country column\n",
    "for i, row in CPI[CPI['Country/Territory'].isnull()].iterrows():\n",
    "    CPI['Country/Territory'][i] = CPI['Country/Territory_y'][i]\n",
    "    \n",
    "# Drop extra country column from merge\n",
    "CPI.drop(labels='Country/Territory_y', axis=1, inplace=True)\n",
    "\n",
    "# Print shape\n",
    "CPI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge 2012 CPI data with overall CPI df on Country column\n",
    "CPI = pd.merge(CPI, CPI2012, how='outer', on='Country/Territory')\n",
    "\n",
    "# Rename columns changed during merge\n",
    "CPI.rename(columns={'Country/Territory_x': 'Country/Territory', 'Region_x': 'Region'}, inplace=True)\n",
    "\n",
    "# Iterate through values that are null within country/territory and copy values from merged Country column\n",
    "for i, row in CPI[CPI['Region'].isnull()].iterrows():\n",
    "    CPI['Region'][i] = CPI['Region_y'][i]\n",
    "\n",
    "# Drop extra region column from merge\n",
    "CPI.drop(labels='Region_y', axis=1, inplace=True)\n",
    "\n",
    "# Print shape\n",
    "CPI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge 2011 CPI data with overall CPI on country column: testCPI\n",
    "testCPI = pd.merge(CPI, CPI2011, how='outer', on='Country/Territory')\n",
    "\n",
    "#testCPI.rename(columns={'Country/Territory_x': 'Country/Territory', 'Region_x': 'Region'}, inplace=True)\n",
    "\n",
    "# # Iterate through values that are null within country/territory and copy values from merged Country column\n",
    "# for i, row in CPI[CPI['Region'].isnull()].iterrows():\n",
    "#     CPI['Region'][i] = CPI['Region_y'][i]\n",
    "    \n",
    "# CPI.drop(labels='Region_y', axis=1, inplace=True)\n",
    "\n",
    "testCPI.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2010 CPI information with overall CPI using country column: testCPI\n",
    "testCPI = pd.merge(testCPI, CPI2010, how='outer', on='Country/Territory')\n",
    "\n",
    "# Print head\n",
    "testCPI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2016-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Progress: export testCPI to csv\n",
    "testCPI.to_csv('./datasets/CPI10-16', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Progress\n",
    "CPI = pd.read_csv('./datasets/CPI10-16')\n",
    "CPI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2009 CPI data with overall CPI df using country column\n",
    "CPI = pd.merge(CPI, CPI2009, how='outer', on='Country/Territory')\n",
    "\n",
    "# Print shape\n",
    "CPI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2008 CPI data with overall CPI df using country column\n",
    "CPI = pd.merge(CPI, CPI2008, how='outer', on='Country/Territory')\n",
    "\n",
    "# Print shape\n",
    "CPI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2007 CPI data with overall CPI df using country column\n",
    "CPI = pd.merge(CPI, CPI2007, how='outer', on='Country/Territory')\n",
    "\n",
    "# Print shape\n",
    "CPI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2006 CPI data with overall CPI df using country column\n",
    "CPI = pd.merge(CPI, CPI2006, how='outer', on='Country/Territory')\n",
    "\n",
    "# Print shape\n",
    "CPI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CPI with 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 2005 CPI data with overall CPI df using country column\n",
    "CPI = pd.merge(CPI, CPI2005, how='outer', on='Country/Territory')\n",
    "\n",
    "# Print shape\n",
    "CPI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint CPI 2005-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Progress: export CPI to csv\n",
    "CPI.to_csv('./datasets/CPI0516', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import checkpoint data\n",
    "CPI = pd.read_csv('./datasets/CPI0516')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country/Territory issue resolution\n",
    "The Country territory is sometimes represented by variations in text, these will need to be manually handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Values that need to be matched with originals\n",
    "CPI[CPI['WB_Code'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below are manual assignments to resolve value issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serbia: Index 217 CPI 2005 score to be copied to Serbia (both: SCG and SRB) at 79,80\n",
    "CPI['CPI_2005_Score'][79] = CPI['CPI_2005_Score'][218]\n",
    "CPI['CPI_2005_Score'][80] = CPI['CPI_2005_Score'][218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiji: Add WB_Code\n",
    "CPI['WB_Code'][217] = 'FJI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moldova: Index 212 to be combined with 107\n",
    "CPI['CPI_2007_Score'][107] = CPI['CPI_2007_Score'][212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kuwait: Index 211 to be combined with 66\n",
    "CPI['CPI_2007_Score'][66] = CPI['CPI_2007_Score'][211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Czech Republic: Index 210 to be combined with 52\n",
    "CPI['CPI_2007_Score'][52] = CPI['CPI_2007_Score'][210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tajikistan: Index 207 to be combined with 157\n",
    "CPI['CPI_2008_Score'][157] = CPI['CPI_2008_Score'][207]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belize : Add WB_Code\n",
    "CPI['WB_Code'][206]= 'BLZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brunei: Index 202 to be combined to 182\n",
    "CPI['CPI_2009_Score'][182] = CPI['CPI_2009_Score'][202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bosnia and Herzegovina: Index 216 to be combined to 83\n",
    "CPI['CPI_2006_Score'][83] = CPI['CPI_2006_Score'][216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congo Democratic Republic: Index 201,209,215 to be combined to 196. Add WB_Code\n",
    "CPI['CPI_2005_Score'][196] = CPI['CPI_2005_Score'][215]\n",
    "CPI['CPI_2006_Score'][196] = CPI['CPI_2006_Score'][215]\n",
    "CPI['CPI_2007_Score'][196] = CPI['CPI_2007_Score'][215]\n",
    "CPI['CPI_2008_Score'][196] = CPI['CPI_2008_Score'][209]\n",
    "CPI['CPI_2009_Score'][196] = CPI['CPI_2009_Score'][201]\n",
    "CPI['CPI_2010_Score'][196] = CPI['CPI_2010_Score'][201]\n",
    "CPI['WB_Code'][196] = 'ZAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congo Republic (not dem): Index 195,200,204,208,213,220 to be combined to 156\n",
    "CPI['CPI_2005_Score'][156] = CPI['CPI_2005_Score'][220]\n",
    "CPI['CPI_2006_Score'][156] = CPI['CPI_2006_Score'][213]\n",
    "CPI['CPI_2007_Score'][156] = CPI['CPI_2007_Score'][213]\n",
    "CPI['CPI_2008_Score'][156] = CPI['CPI_2008_Score'][208]\n",
    "CPI['CPI_2009_Score'][156] = CPI['CPI_2009_Score'][204]\n",
    "CPI['CPI_2010_Score'][156] = CPI['CPI_2010_Score'][200]\n",
    "CPI['CPI_2011_Score'][156] = CPI['CPI_2011_Score'][195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vietnam: Index 194 to be combined to 127\n",
    "CPI['CPI_2006_Score'][127] = CPI['CPI_2006_Score'][194]\n",
    "CPI['CPI_2007_Score'][127] = CPI['CPI_2007_Score'][194]\n",
    "CPI['CPI_2008_Score'][127] = CPI['CPI_2008_Score'][194]\n",
    "CPI['CPI_2011_Score'][127] = CPI['CPI_2011_Score'][194]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiribati: Add WB_code\n",
    "CPI['WB_Code'][193] = 'KIR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tonga: Add WB_code\n",
    "CPI['WB_Code'][192] = 'TON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanuatu: Add WB_code\n",
    "CPI['WB_Code'][191] = 'VUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macau: Index 198 to be combined to 190\n",
    "CPI['CPI_2006_Score'][190] = CPI['CPI_2006_Score'][198]\n",
    "CPI['CPI_2007_Score'][190] = CPI['CPI_2007_Score'][198]\n",
    "CPI['CPI_2008_Score'][190] = CPI['CPI_2008_Score'][198]\n",
    "CPI['CPI_2009_Score'][190] = CPI['CPI_2009_Score'][198]\n",
    "CPI['CPI_2010_Score'][190] = CPI['CPI_2010_Score'][198]\n",
    "CPI['WB_Code'][190] = 'MAC'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# South Korea: Index 189 to be combined to 42\n",
    "CPI['CPI_2005_Score'][42] = CPI['CPI_2005_Score'][189]\n",
    "CPI['CPI_2006_Score'][42] = CPI['CPI_2006_Score'][189]\n",
    "CPI['CPI_2007_Score'][42] = CPI['CPI_2007_Score'][189]\n",
    "CPI['CPI_2008_Score'][42] = CPI['CPI_2008_Score'][189]\n",
    "CPI['CPI_2011_Score'][42] = CPI['CPI_2011_Score'][189]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# North Korea: Index 197 to be combined to 179\n",
    "CPI['CPI_2011_Score'][179] = CPI['CPI_2011_Score'][197]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA: Index 188 to be combined to 19\n",
    "CPI['CPI_2005_Score'][19] = CPI['CPI_2005_Score'][188]\n",
    "CPI['CPI_2006_Score'][19] = CPI['CPI_2006_Score'][188]\n",
    "CPI['CPI_2007_Score'][19] = CPI['CPI_2007_Score'][188]\n",
    "CPI['CPI_2008_Score'][19] = CPI['CPI_2008_Score'][188]\n",
    "CPI['CPI_2011_Score'][19] = CPI['CPI_2011_Score'][188]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macedonia: Index 187, 199, 205 to be combined to 64\n",
    "CPI['CPI_2005_Score'][64] = CPI['CPI_2005_Score'][187]\n",
    "CPI['CPI_2006_Score'][64] = CPI['CPI_2006_Score'][187]\n",
    "CPI['CPI_2007_Score'][64] = CPI['CPI_2007_Score'][199]\n",
    "CPI['CPI_2008_Score'][64] = CPI['CPI_2008_Score'][205]\n",
    "CPI['CPI_2009_Score'][64] = CPI['CPI_2009_Score'][199]\n",
    "CPI['CPI_2010_Score'][64] = CPI['CPI_2010_Score'][199]\n",
    "CPI['CPI_2011_Score'][64] = CPI['CPI_2011_Score'][187]\n",
    "CPI['CPI_2012_Score'][64] = CPI['CPI_2012_Score'][187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cote d'Ivoire: Index 119, 203, 214 to be combined to 118\n",
    "CPI['CPI_2005_Score'][118] = CPI['CPI_2005_Score'][214]\n",
    "CPI['CPI_2007_Score'][118] = CPI['CPI_2007_Score'][214]\n",
    "CPI['CPI_2008_Score'][118] = CPI['CPI_2008_Score'][203]\n",
    "CPI['CPI_2009_Score'][118] = CPI['CPI_2009_Score'][203]\n",
    "CPI['CPI_2010_Score'][118] = CPI['CPI_2010_Score'][119]\n",
    "CPI['CPI_2011_Score'][118] = CPI['CPI_2011_Score'][119]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of countries that do not have unique WB_Code\n",
    "* As we have manually merged the incorrectly labeled data, we can now delete all duplicated country data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print shape and head\n",
    "print(CPI.shape)\n",
    "CPI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign copy of CPI to testCPI\n",
    "testCPI = CPI\n",
    "\n",
    "# Print shape\n",
    "testCPI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of countries with null values in WB_Code and drop from CPI dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate empty list for appending\n",
    "rowstodrop = []\n",
    "\n",
    "# For each row of data that contains a null value in the WB_Code column\n",
    "for i, row in CPI[CPI['WB_Code'].isnull()].iterrows():\n",
    "    # Append the index to rowstodrop\n",
    "    rowstodrop.append(i)\n",
    "    \n",
    "# Drop all rows that were appended to rowstodrop\n",
    "CPI.drop(labels=rowstodrop, inplace=True)\n",
    "\n",
    "# Print shape\n",
    "CPI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CPI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPI: Completed File\n",
    "Export the compiled Corruption Perception Index dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Progress: Export complete compiled CPI csv\n",
    "CPI.to_csv('./datasets/CPI_Total.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary EDA\n",
    "* In order to make some preliminary observations on the data, the number of missing values will be graphed to ascertain how much data we actually have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.ylabel('Number of Missing Values')\n",
    "plt.xlabel('Year')\n",
    "plt.title('Missing Values by Year')\n",
    "plt.grid(b=True, which='both')\n",
    "\n",
    "# Plotting sum of missing values on same plot to compare values of indicators\n",
    "missEleAcc.plot(figsize=[18,10], legend=True, label='Electricity Access')\n",
    "missElec.plot(legend=True, label='Elec Consumption')\n",
    "missEmis.plot(legend=True, label='Emissions')\n",
    "missFossil.plot(legend=True, label='Fossil Fuel Cons')\n",
    "missgdp.plot(legend=True, label='GDP')\n",
    "missGDPpc.plot(legend=True, label='GDP Per Cap')\n",
    "missInfant.plot(legend=True, label='Infant Mortality')\n",
    "missInfl.plot(legend=True, label='Inflation')\n",
    "missLE.plot(legend=True, label='Life Expectancy')\n",
    "missWomenProp.plot(legend=True, label='Prop of Women ParlT')\n",
    "missSanit.plot(legend=True, label='Sanitation Access')\n",
    "missTaxComm.plot(legend=True, label='Commercial Tax')\n",
    "missElec_OGC.plot(legend=True, label='Elec from Oil/gas/coal')\n",
    "missOreMetals.plot(legend=True, label='Ore Metals export')\n",
    "missTechExp.plot(legend=True, label='Tech Exports')\n",
    "missBusReg.plot(legend=True, label='Procedure Bus register')\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Findings\n",
    "* It is seen that there are a number of indicators that are not collected as well as others.\n",
    "* Data appears to be the most complete mid-2000's onwards\n",
    "* We will need to subset our data from 2005 onwards to minimize imputation"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
